import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os

# ==============================
# é…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
# ==============================
CONFIG = {
    "base_model": "EleutherAI/pythia-1b",
    "output_dir": "./results",
    "use_peft": True,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "load_in_8bit": False,
    "load_in_4bit": False,
}

# æ¨¡å‹ä¿å­˜è·¯å¾„
models_dir = f"{CONFIG['output_dir']}/models"
sft_model_path = f"{models_dir}/sft_model"

# ==============================
# å‘½ä»¤è¡Œå‚æ•°
# ==============================
parser = argparse.ArgumentParser(description="SFT æ¨¡å‹å“åº”ç”Ÿæˆå™¨")
parser.add_argument("-prompt", type=str, required=True, help="ç”¨æˆ·è¾“å…¥çš„æç¤º")
args = parser.parse_args()

# ==============================
# åŠ è½½ Tokenizer
# ==============================
tokenizer = AutoTokenizer.from_pretrained(sft_model_path)

# ==============================
# åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®æ˜¯å¦PEFTåŠ è½½ï¼‰
# ==============================
print("ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹...")
if CONFIG["use_peft"]:
    base_model = AutoModelForCausalLM.from_pretrained(CONFIG["base_model"]).to(CONFIG["device"])
    model = PeftModel.from_pretrained(base_model, sft_model_path).to(CONFIG["device"])
else:
    model = AutoModelForCausalLM.from_pretrained(sft_model_path).to(CONFIG["device"])

model.eval()

# ==============================
# æ„é€ è¾“å…¥æç¤º
# ==============================
prompt = args.prompt
formatted_prompt = f"### Instruction:\n{prompt}\n\n### Response:\n"

# ç¼–ç è¾“å…¥
inputs = tokenizer(formatted_prompt, return_tensors="pt").to(CONFIG["device"])

# ç”Ÿæˆé…ç½®
gen_config = {
    "max_new_tokens": 150,
    "do_sample": True,
    "temperature": 0.7,
    "top_p": 0.9,
}

# ==============================
# ç”Ÿæˆå›å¤
# ==============================
print("ğŸ§  æ­£åœ¨ç”Ÿæˆå›å¤...\n")
with torch.no_grad():
    output = model.generate(**inputs, **gen_config)

response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print("ğŸ¤– æ¨¡å‹å›å¤:")
print(response)
